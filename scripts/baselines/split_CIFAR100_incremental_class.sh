OUTDIR=outputs/split_CIFAR100_incremental_class
REPEAT=5
mkdir -p ${OUTDIR}/Offline_SGD_WideResNet_28_2_cifar
mkdir -p ${OUTDIR}/Offline_Adam_WideResNet_28_2_cifar
mkdir -p ${OUTDIR}/Adam
mkdir -p ${OUTDIR}/SGD
mkdir -p ${OUTDIR}/Adagrad
python -u iBatchLearn.py --dataset CIFAR100 --train_aug --gpuid 0 --repeat $REPEAT --incremental_class --optimizer SGD     --force_out_dim 100 --no_class_remap --first_split_size 10 --other_split_size 10 --schedule 80 120 160 --batch_size 128 --model_name WideResNet_28_2_cifar --model_type resnet                        --lr 0.1 --momentum 0.9 --weight_decay 1e-4 --offline_training  | tee ${OUTDIR}/Offline_SGD_WideResNet_28_2_cifar/experiment.log     &    
python -u iBatchLearn.py --dataset CIFAR100 --train_aug --gpuid 0 --repeat $REPEAT --incremental_class --optimizer Adam    --force_out_dim 100 --no_class_remap --first_split_size 10 --other_split_size 10 --schedule 80 120 160 --batch_size 128 --model_name WideResNet_28_2_cifar --model_type resnet                        --lr 0.001                                  --offline_training  | tee ${OUTDIR}/Offline_Adam_WideResNet_28_2_cifar/experiment.log    &
python -u iBatchLearn.py --dataset CIFAR100 --train_aug --gpuid 1 --repeat $REPEAT --incremental_class --optimizer Adam    --force_out_dim 100 --no_class_remap --first_split_size 10 --other_split_size 10 --schedule 80 120 160 --batch_size 128 --model_name WideResNet_28_2_cifar --model_type resnet                                             --lr 0.001                                 | tee ${OUTDIR}/Adam/experiment.log                                  &
python -u iBatchLearn.py --dataset CIFAR100 --train_aug --gpuid 2 --repeat $REPEAT --incremental_class --optimizer SGD     --force_out_dim 100 --no_class_remap --first_split_size 10 --other_split_size 10 --schedule 80 120 160 --batch_size 128 --model_name WideResNet_28_2_cifar --model_type resnet                                             --lr 0.1                                   | tee ${OUTDIR}/SGD/experiment.log                                   &
python -u iBatchLearn.py --dataset CIFAR100 --train_aug --gpuid 3 --repeat $REPEAT --incremental_class --optimizer Adagrad --force_out_dim 100 --no_class_remap --first_split_size 10 --other_split_size 10 --schedule 80 120 160 --batch_size 128 --model_name WideResNet_28_2_cifar --model_type resnet                                             --lr 0.1                                   | tee ${OUTDIR}/Adagrad/experiment.log                               &
